---
title: Reproducibility
author: Jens von Bergmann
date: '2017-09-09'
slug: reproducibility
categories:
  - cancensus
  - CensusMapper
tags: []
draft: true
description: "Moving the discussion forward"
featured: ''
featuredalt: ""
featuredpath: ""
linktitle: ''
type: "post"
---



<p>CensusMapper has come a long way, in the latest iteration we opened up an <a href="https://censusmapper.ca/api">API for convenient and pinpointed census data access for everyone</a>. It’s a step that was overdue. The CensusMapper concept is built entirely around APIs, but they were geared toward mapping needs. With changes to only a couple of lines of code we adapted these to be useful for more general data needs.</p>
<p>But more importantly for us, this small change has already had a huge impact of how we at CensusMapper handle data analysis internally.</p>
<div id="why-is-the-api-such-a-big-deal" class="section level2">
<h2>Why is the API such a big deal?</h2>
<p>An “Application Programming Interface” is a formal protocol that specifies how different processes should interact. The CensusMapper API provides an stable, predictable and convenient interface how to access census data.</p>
<p>In the past we went staight into the CensusMapper database if we wanted to work with census data. In plain language, if we wanted a certain subset of the data to work with, we wrote a line or two of SQL and extracted a CSV or JSON dump for further processing. This works great if you have direct access to the database and is maximally flexible. But it also means that the data extracts lack a consistency. And that means that the scripts we used for data processing, analysis and visualization always needed soem adjustments to work with a new data extract. And that introduces friction. The API changes that. Now every data extract is formatted in the exact same way and our processing tools are reusable out of the box. The API has already made our workflows <em>a lot</em> more efficient.</p>
</div>
<div id="opening-up-the-api" class="section level2">
<h2>Opening up the API</h2>
<p>Then we opened up the API. That forced us to further formalise the protocol. And we also initiated a <a href="https://github.com/mountainMath/cancensus">public R package</a> that acts as glue between R, a popular platform for statistical analysis, and the CensusMapper API. This enables anyone to uses these tools. And to contribute to their development, which people did. Open source can be a powerful tool to drive innovation and ensure high quality, and this is exactly what happend with the <code>cancensus</code> R package. Through the collaborative work of others <code>cancensus</code> development was much faster than I could have done it, and most importantly, it became a much better package than I could have done on my own.</p>
<p>I have been an advocate for open data and open source for quite some time now, but it was amazing to see this at work at some of my own products.</p>
</div>
<div id="cancensus" class="section level2">
<h2><code>cancensus</code></h2>
<p>The API on it’s own is already quite useful. One can easily download census data in CSV format, and the geographic data in geojson. Part of the CensusMapper paradigm dictates that the geographic data and the census data need to remain separate, so the user will have to join these after accessing the data. When mapping data, CensusMapper takes care of this gluing operation in a way that is completely opaque to the user. Someone viewing CensusMapper maps would not know that each map tiles requires two API calls to display the data. One for the geographic data and one for the census variables. Similarly, the <code>cancensus</code> R package takes care of this gluing operation internally, the users just needs to specify what data formate they want the geographic data in. Or that they don’t need geographic data for a particular analysis they do, although this is generally not advisable for various reasons when working with census data.</p>
</div>
<div id="reproducibility" class="section level2">
<h2>Reproducibility</h2>
<p>Reproducibility is a big word. In this context I mean the ability to reproduce an analysis or visualization. Internally at MountainMath, all our analysis is done via script, so we can re-run it at will and get the same result. This includes the extraction of the data out of the CensusMapper database in case we use census data. In that sense, pretty much everything we do is reporducible. By us. But not by other people, since they don’t have internal access to our database. And if we were a bigger shop with more people and various access privileges, it probably would not be reproducible by all people in the company.</p>
<p>The API changes that. There are still restrictions at the API level, some datasets we have come with access restrictions that only select users can access. But a good portion is, and any analysis based on that data can now easily be shared. And access permissions are easily managed ot open up other portions as needed.</p>
</div>
<div id="open-access" class="section level2">
<h2>Open Access</h2>
<p>Reproducibility fits neatly into the open access paradigm. Academia is slowly moving in that direction. In Canada, researchers on tri-council grants are now required to publish there results in open-access journals. And while the transition is slow, the direction is clear and the push (and <a href="https://twitter.com/Hulchanski/status/899798043557584896">public shaming</a>) to publish in open access journals gets louder. Nobody is perfect, and I admit that in my past academic life I have published in <a href="https://msp.org/gt/2007/11-1/p01.xhtml">open access journals</a> as well as <a href="https://link.springer.com/article/10.1007%2Fs10455-010-9195-3?LI=true">deep in evil empire territory</a>.</p>
<p>What has received less attention is that tri-council grants also require that any data aquired via the grant be made public. And while acadmics are generally quite willing to email out a pdf of their research published in paywalled journals, I have had very little success compelling them to share raw data unless there are valid legal objections to do so.</p>
<p>But increasingly one can find data and, if appropriate, code published together with research articles. One example of this is a recent <a href="https://milescorak.com">interesting study by Miles Croak on Economic Opportunity in Canada</a> that makes the data used in the study available. In cases like this, where the data is derived from a StatCan custom tabulation, providing a clean spreadsheet with detailed explanation is the best possible way to share the data.</p>
<p>But too often do people publish analysis results with simply citing the standard census release data from StatsCanada as the data source. I have spent countless hours trying to reproduce other people’s work by guessing exactly what variables were used, how the analysis was perform using what statistic and still could not get the results to match. In important cases I have contacted people who mostly were more than willing to share further details, but this process is less than ideal.</p>
</div>
<div id="analysis-code-data-api" class="section level2">
<h2>Analysis + Code + <del>Data</del> API</h2>
<p>Ideally an analysis should be reproducible by anyone. Traditionally that meant explaining the methods used in great detail, often providing the formulas in the printed publication. This introduces all kinds of friction, from typos in the formula to necessarily jumping steps and incomplete descriptions, to the hassle of having to rewrite all the code in order to reproduce the analysis. Sharing the code, and the data, removed these barriers. But modern datasets are often massive. Even census data, which is sizeable with about half a billion fields per standard census data release, but wouldn’t be considered “big” data by today’s standards, becomes somewhat unwieldly when shared. Plus it becomes impossible to verify the authenticity of the shared data.</p>
<p>That’s where APIs come in, they remove the need to share the data, the code calling the API is enough. And because API calls can pinpoint the requierd data much better one only grabs the data needed for the analysis instead of downloading a large census dataset and then cutting it down to the required chunk.</p>
<p>The CensusMapper API can provide this service, with the exception that it is not an “authorative” source. We have spent quite some time to verify the accuracy of our data imports and that the API calls accurately reproduce StatCan data, but, as much as I hate to admit it, we lack the authority that StatCan has (even though StatCan themselves are not free of data quality issues). Ideally the API that we have would be built and hosted by StatCan. And while StatCan has for years promised to deliver just such an API under their “New Dissemination Model”, this has failed to materialize and prompted us to open up ours. StatCan would also have the resources to remove API quotas to unlock the full potential of such an API. That will never remove the need for third party APIs like ours, that will also offer processed data like our common tiling offering on 2011 and 2016 data on the same geographies down to the dissemination block level. To the contrary, I expect it would accelerate the development of more third party APIs as it would make it a lot simple to build derived products.</p>
</div>
<div id="fully-reproducible-analysis" class="section level2">
<h2>Fully reproducible analysis</h2>
<p>At CensusMapper we love maps. As much as we hate to admit it, a map isn’t an analysis. It may facilitate the communication of the result of an analysis, but it does not replace it. In some cases, for example our <a href="https://censusmapper.ca/maps/142">Diversity Index Map</a> the map performs it’s own mini-analysis on the fly, it computes the diversity index based on the ethnicities in each area and displays the result. The map story gives some context. A more complex example is our Surprise Maps that dynamically perform some statistical hypothesis testing <a href="https://doodles.mountainmath.ca/blog/2017/04/10/surprise/">as explained in more detail in a previous post</a>. But while these can count in some limited way as analysis, the more general point remains that maps aren’t analysis.</p>
<p>For our outward facing mini (or sometimes not so mini) analysis here on our blog we have now migrated our blogging platform to Hugo-based <a href="https://github.com/rstudio/blogdown"><code>blogdown</code></a>, which means our blogposts are now writting in R markdown. And it has immediately transformed how we write our blog posts. The analysis is now embedded directly into the post. Code, explanations, visualizations are all part of one and the same document. For legibility we do suppress the displaying of some of the code snippets, trying to strike a balance between general readability and providing details to the more technically minded reader. The codes blocks are clearly identifiable as such, and I expect the average reader to skip right over them, while some readers that want to know in more detail how our analysis was performed can easily get that information. And anyone that wants to check, expand or modify the analysis can just download the R markdown, view the suppressed code blocks, and immediately re-run the analysis on their machine and make whatever changes they want.</p>
<p>Even simple things like the numbers in our post are actually just the result of computations done in R, eliminating all friction between analysis and the report. {{% figure src="/images/r_numbers.png" title="R markdown code" %}}</p>
{{% figure src="/images/html_numbers.png" title="Blog Post Output" %}}
<p>In short, our blog posts and out analysis is one and the same thing, although they still reside in different places. The blog is served as HTML at out <a href="https://doodles.mountainmath.ca">doodles.mountainmath.ca</a>, and the R markdown resides on out public <a href="https://github.com/mountainMath/doodles/tree/master/content/posts">github repository</a>. That separation just technical though, our html website is a derived product. Updating the github repo is what triggers an automated generation of html and deployment to our website via <a href="https://www.netlify.com">netlify</a>, so the two will never be out of sync. The github repo keeps the entire history of commits, reflecting all updates to the blog and providing full transparancy.</p>
</div>
<div id="moving-the-discussion-forward" class="section level2">
<h2>Moving the discussion forward</h2>
<p>We believe that this is the process that is needed nowadays to “move the discussion forward”. In our hard to navigate information age we too often end up being torn between opposing opinions with no way to evaluate them, having to resolve to choose whom to trust. People trump up their credentials and titles to end up ahead in the battle for trust.</p>
<p>One way out of this is reproducible analysis. This point of view values the analysis itself over the author, it pitches the analysis as a stepping stone toward greater insight. Platforms like github can serve as collaborative environments where people can discuss issues they find with particular parts of an analysis and collaboratively solve them. Or fork the analysis and move it further on their own. Not everyone has the ability to do this, but enough people do. And one does not have to be an expert to do this, but if only takes a moderate technical skill level to download <a href="https://www.rstudio.com">RStudio</a>, grab the R markdown file from the web and run it. It will likely require the person to install some packages to make it work, but it is a rather painless procedure. And one does not have to be an expert in R to quickly get a grasp what what is going on and make minor modifications to play with the data.</p>
</div>
<div id="the-vancouver-context" class="section level2">
<h2>The Vancouver Context</h2>
<p>In Vancouver we have heard cries for more data. And data is lacking for many needed analyses, and we need to keep pushing for the release of more and better data. But what’s more needed than data is people to analyse data and turn it into information, to provide answers to questions.</p>
</div>
<div id="other-api-tie-ins" class="section level2">
<h2>Other API tie-ins</h2>
<p>One can easily imaging a package similar to <code>cancensus</code> for Python to facilitate the access to census data for people that prefer python over R for geospatial analysis. Or a plugin for QGis or ArcGis for people more comfortable with desktop GIS software. Sadly, my resources are stretched quite thin as things are and my need for this is minimal right now. But with the API available anyone what does have a need could build these fairly easily.</p>
</div>
